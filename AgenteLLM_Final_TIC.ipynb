{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a625763-04b5-4a40-8b24-7e74412e6804",
   "metadata": {},
   "source": [
    "Requisitos (instalar antes)\n",
    "\n",
    "Tener en cuenta que para el uso maximo con gpu es necesario instalar cuda con la version de python y drivers de la gráfica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7250b6-09fd-4585-8211-4e01c3c1f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U rank-bm25 sentence-transformers torch faiss-cpu numpy llama-cpp-python gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbb08d-3aee-475b-a54c-6754caeb9e4a",
   "metadata": {},
   "source": [
    "Descargar modelo mistral Q4 K M de 4.37gb desde el navegador escribiendo la url* https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF *\n",
    "Ubicar en la misma carpeta que este archivo ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9ca421-365c-4d4b-89d4-65ce9ae53675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] DEVICE = cuda\n",
      "[TIMING][build_embeddings] total=7.750s\n",
      "[OK] Índices listos en 12.68s. Docs: 26 | LLM=ON | Rerank=ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anddy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\components\\dataframe.py:191: UserWarning: The `col_count` parameter is deprecated and will be removed. Please use `column_count` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAG híbrido TIC \n",
    "# - Cita automática de evidencias usadas en la respuesta: [E1] [E3] ...\n",
    "# - Comandos (desde la MISMA GUI):\n",
    "#     :help\n",
    "#     :mode agent|eval\n",
    "#     :facet soft|strict\n",
    "#     :facets on|off\n",
    "#     :rerank on|off\n",
    "#     :gen on|off\n",
    "#     :exit\n",
    "# ============================================================\n",
    "\n",
    "import sys, subprocess, importlib, json, re, unicodedata, time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# -------------------- seguridad / límites --------------------\n",
    "SAFE_CTX_MAX = 5200   # tamaño máximo de contexto como seguridad\n",
    "SAFE_CTX_MIN = 800\n",
    "\n",
    "def clamp_int(x, lo, hi, default):\n",
    "    try:\n",
    "        v = int(x)\n",
    "    except Exception:\n",
    "        return int(default)\n",
    "    if v < lo:\n",
    "        return int(lo)\n",
    "    if v > hi:\n",
    "        return int(hi)\n",
    "    return int(v)\n",
    "\n",
    "# -------------------- deps --------------------\n",
    "def ensure(pkg, pip_name=None):\n",
    "    try:\n",
    "        return importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", (pip_name or pkg), \"-q\"])\n",
    "        return importlib.import_module(pkg)\n",
    "\n",
    "rank_bm25 = ensure(\"rank_bm25\", \"rank-bm25\")\n",
    "st = ensure(\"sentence_transformers\", \"sentence-transformers\")\n",
    "torch = ensure(\"torch\", \"torch\")\n",
    "numpy_mod = ensure(\"numpy\", \"numpy\")\n",
    "faiss = ensure(\"faiss\", \"faiss-cpu\")\n",
    "gradio = ensure(\"gradio\", \"gradio\")\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# llama.cpp\n",
    "try:\n",
    "    llama_cpp = ensure(\"llama_cpp\", \"llama-cpp-python\")\n",
    "    from llama_cpp import Llama\n",
    "except Exception:\n",
    "    Llama = None\n",
    "\n",
    "# ============================ paths / modelos ============================\n",
    "\n",
    "JSON_PATH = r\"estructura_semantica_final_PTIC2.json\"\n",
    "GGUF_PATH = r\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "PROMPT_PATH_AGENT = r\"prompt_base.txt\"\n",
    "PROMPT_PATH_EVAL = r\"prompt_base_ev.txt\"\n",
    "\n",
    "EMB_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "# llama cfg\n",
    "N_GPU_LAYERS = 5\n",
    "N_THREADS = 8\n",
    "N_CTX = 4096\n",
    "MAX_TOKENS = 384\n",
    "TEMPERATURE_AGENT = 0.2\n",
    "TEMPERATURE_EVAL = 0.0\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[RAG] DEVICE = {DEVICE}\")\n",
    "\n",
    "engine = None  # global (se comparte con la GUI)\n",
    "\n",
    "# ============================ utils ============================\n",
    "\n",
    "def norm(s: Any) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    if isinstance(s, list):\n",
    "        s = \" \".join(map(str, s))\n",
    "    s = str(s).lower().strip()\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(c for c in s if unicodedata.category(c) != \"Mn\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    toks = s.split()\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        out.append(t[:-1] if (len(t) > 3 and t.endswith(\"s\")) else t)\n",
    "    return \" \".join(out)\n",
    "\n",
    "def tokenize_simple(s: str) -> List[str]:\n",
    "    return re.findall(r\"[a-z0-9]+\", norm(s))\n",
    "\n",
    "def char_trigrams(s: str) -> set:\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = f\" {s} \"\n",
    "    if len(s) < 3:\n",
    "        return {s}\n",
    "    return {s[i:i+3] for i in range(len(s)-2)}\n",
    "\n",
    "def jaccard_trigram(a: str, b: str) -> float:\n",
    "    A, B = char_trigrams(a), char_trigrams(b)\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def jaccard_tokens(a: str, b: str) -> float:\n",
    "    A, B = set(tokenize_simple(a)), set(tokenize_simple(b))\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def dedup_type_name(tipo: Optional[str], nombre: Optional[str]) -> Tuple[str, str]:\n",
    "    t = (tipo or \"\").strip()\n",
    "    n = (nombre or \"\").strip()\n",
    "    if not n and not t:\n",
    "        return \"\", \"\"\n",
    "    tn = norm(t)\n",
    "    nn = norm(n)\n",
    "    base = n\n",
    "    if tn and (nn.startswith(tn + \" \") or nn == tn):\n",
    "        if n.lower().startswith(t.lower()):\n",
    "            base = n[len(t):].lstrip()\n",
    "    display = (f\"{t} {base}\".strip() if t else base).strip()\n",
    "    return base, display\n",
    "\n",
    "CODE_RE = re.compile(r\"\\b[a-z]?\\d{2,4}[a-z\\d]*\\b\", re.IGNORECASE)\n",
    "\n",
    "# ============================ carga docs ============================\n",
    "\n",
    "def load_docs(json_path: str | Path) -> List[Dict[str, Any]]:\n",
    "    data = json.load(open(json_path, \"r\", encoding=\"utf-8\"))\n",
    "    docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    def flatten_doc(d: Dict[str, Any], prefix: str = \"\", out: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        if out is None:\n",
    "            out = {}\n",
    "        for k, v in (d or {}).items():\n",
    "            if str(k).startswith(\"_\"):\n",
    "                continue\n",
    "            path = f\"{prefix}.{k}\" if prefix else str(k)\n",
    "            if isinstance(v, dict):\n",
    "                flatten_doc(v, path, out)\n",
    "            elif isinstance(v, list):\n",
    "                out[path] = v\n",
    "            else:\n",
    "                out[path] = v\n",
    "        return out\n",
    "\n",
    "    for e in data.get(\"espacios\", []):\n",
    "        attrs = e.get(\"attrs\", {}) or {}\n",
    "        personal = []\n",
    "        for p in (attrs.get(\"Personal\", []) or []):\n",
    "            nm = p.get(\"ConNombre\") or p.get(\"nombre\") or p.get(\"Encargado\")\n",
    "            if nm:\n",
    "                personal.append(str(nm))\n",
    "\n",
    "        bloque = e.get(\"bloque\") or e.get(\"bloque_id\") or \"\"\n",
    "        if isinstance(bloque, str) and bloque.startswith(\"BL-\"):\n",
    "            bloque = bloque.replace(\"BL-\", \"\")\n",
    "        if isinstance(bloque, str) and bloque.isdigit():\n",
    "            bloque = f\"A{bloque}\"\n",
    "\n",
    "        pieza = {\n",
    "            \"id\": str(e.get(\"id\") or e.get(\"codigo\") or e.get(\"nombre\")),\n",
    "            \"kind\": \"espacio\",\n",
    "            \"codigo\": e.get(\"codigo\"),\n",
    "            \"tipo\": e.get(\"tipo\"),\n",
    "            \"nombre\": e.get(\"nombre\"),\n",
    "            \"bloque\": bloque,\n",
    "            \"piso\": attrs.get(\"piso\"),\n",
    "            \"direccionRelativa\": attrs.get(\"direccionRelativa\"),\n",
    "            \"direccionOrientativa\": attrs.get(\"direccionOrientativa\"),\n",
    "            \"carrera\": attrs.get(\"AsignadoACarrera\"),\n",
    "            \"facultad\": attrs.get(\"Facultad\") or attrs.get(\"facultad\"),\n",
    "            \"capacidad\": attrs.get(\"ConCapacidad\"),\n",
    "            \"encargados\": personal,\n",
    "            \"aliases\": [str(a) for a in (e.get(\"aliases\") or [])],\n",
    "            \"attrs\": attrs,\n",
    "        }\n",
    "\n",
    "        fields_raw = [\n",
    "            pieza.get(\"tipo\"),\n",
    "            pieza.get(\"nombre\"),\n",
    "            pieza.get(\"codigo\"),\n",
    "            str(bloque),\n",
    "            (f\"piso {pieza.get('piso')}\" if pieza.get(\"piso\") is not None else \"\"),\n",
    "            pieza.get(\"direccionRelativa\"),\n",
    "            pieza.get(\"direccionOrientativa\"),\n",
    "            pieza.get(\"carrera\"),\n",
    "            pieza.get(\"facultad\"),\n",
    "            pieza.get(\"capacidad\"),\n",
    "            \" \".join(pieza.get(\"aliases\") or []),\n",
    "            \" \".join(pieza.get(\"encargados\") or []),\n",
    "            json.dumps(pieza.get(\"attrs\", {}), ensure_ascii=False),\n",
    "        ]\n",
    "        pieza[\"_raw_text\"] = \" \".join([str(x) for x in fields_raw if x])\n",
    "        pieza[\"_fulltext\"] = norm(\" \".join([str(x) for x in fields_raw if x]))\n",
    "\n",
    "        _, name_display = dedup_type_name(pieza.get(\"tipo\"), pieza.get(\"nombre\"))\n",
    "        pieza[\"_name_norm\"] = norm(name_display)\n",
    "        pieza[\"_aliases_norm\"] = [norm(a) for a in (pieza.get(\"aliases\") or [])]\n",
    "\n",
    "        code_ft = norm(pieza.get(\"codigo\") or \"\")\n",
    "        name_ft = norm(name_display)\n",
    "        aliases_ft = norm(\" \".join(pieza.get(\"aliases\") or []))\n",
    "        encargados_ft = norm(\" \".join(pieza.get(\"encargados\") or []))\n",
    "        rest_ft = pieza[\"_fulltext\"]\n",
    "\n",
    "        def rep(text, k):\n",
    "            return \" \".join([text] * k) if text else \"\"\n",
    "\n",
    "        pieza[\"_bm25_text\"] = \" \".join([\n",
    "            rep(code_ft, 6),\n",
    "            rep(name_ft, 4),\n",
    "            rep(encargados_ft, 4),\n",
    "            rep(aliases_ft, 3),\n",
    "            rest_ft,\n",
    "        ]).strip()\n",
    "\n",
    "        pieza[\"_flat_attrs\"] = flatten_doc({\"attrs\": attrs})\n",
    "        docs.append(pieza)\n",
    "\n",
    "    return docs\n",
    "\n",
    "def collect_attr_catalog(docs: List[Dict[str, Any]]) -> Dict[str, set]:\n",
    "    values: Dict[str, set] = {}\n",
    "    for d in docs:\n",
    "        if d.get(\"kind\") != \"espacio\":\n",
    "            continue\n",
    "        for path, val in (d.get(\"_flat_attrs\") or {}).items():\n",
    "            if path.startswith(\"_\"):\n",
    "                continue\n",
    "            values.setdefault(path, set())\n",
    "\n",
    "            def add_val(x):\n",
    "                s = norm(x)\n",
    "                if s:\n",
    "                    values[path].add(s)\n",
    "\n",
    "            if isinstance(val, list):\n",
    "                for x in val:\n",
    "                    add_val(x)\n",
    "            elif isinstance(val, dict):\n",
    "                for k in val.keys():\n",
    "                    add_val(k)\n",
    "            else:\n",
    "                add_val(val)\n",
    "    return values\n",
    "\n",
    "# ============================ índices ============================\n",
    "\n",
    "def build_bm25(docs: List[Dict[str, Any]]):\n",
    "    tokens_all = [d[\"_bm25_text\"].split() for d in docs]\n",
    "    return BM25Okapi(tokens_all)\n",
    "\n",
    "def build_embeddings(docs: List[Dict[str, Any]], model_name: str, M: int = 32):\n",
    "    t0 = time.time()\n",
    "    model = SentenceTransformer(model_name, device=DEVICE)\n",
    "    corpus = [d[\"_raw_text\"] for d in docs]\n",
    "    vecs = model.encode(corpus, batch_size=64, normalize_embeddings=True, show_progress_bar=False)\n",
    "    vecs = np.asarray(vecs, dtype=\"float32\")\n",
    "    d = vecs.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_INNER_PRODUCT)\n",
    "    faiss.normalize_L2(vecs)\n",
    "    try:\n",
    "        index.hnsw.efConstruction = 200\n",
    "    except Exception:\n",
    "        pass\n",
    "    index.add(vecs)\n",
    "    print(f\"[TIMING][build_embeddings] total={(time.time() - t0):.3f}s\")\n",
    "    return model, index\n",
    "\n",
    "# ============================ facets ============================\n",
    "\n",
    "TIPO_SYNONYMS = {\n",
    "    \"lab\": \"laboratorio\", \"laboratorio\": \"laboratorio\", \"laboratorios\": \"laboratorio\",\n",
    "    \"sala\": \"sala\", \"salas\": \"sala\",\n",
    "    \"oficina\": \"oficina\", \"oficinas\": \"oficina\",\n",
    "    \"departamento\": \"departamento\", \"departamentos\": \"departamento\",\n",
    "    \"ducto\": \"ducto\", \"ductos\": \"ducto\",\n",
    "    \"cuarto\": \"cuarto\", \"cuartos\": \"cuarto\",\n",
    "    \"infraestructuravertical\": \"infraestructuravertical\",\n",
    "    \"escalera\": \"infraestructuravertical\", \"escaleras\": \"infraestructuravertical\",\n",
    "    \"ascensor\": \"infraestructuravertical\", \"ascensores\": \"infraestructuravertical\",\n",
    "    \"bano\": \"baño\", \"banio\": \"baño\", \"banos\": \"baño\", \"banios\": \"baño\",\n",
    "    \"baño\": \"baño\", \"baños\": \"baño\",\n",
    "    \"sshh\": \"baño\", \"ssh\": \"baño\", \"wc\": \"baño\",\n",
    "    \"servicio\": \"baño\", \"servicios\": \"baño\",\n",
    "    \"higienico\": \"baño\", \"higienicos\": \"baño\",\n",
    "    \"sanitario\": \"baño\", \"sanitarios\": \"baño\",\n",
    "}\n",
    "\n",
    "def parse_piso(text: str) -> Optional[int]:\n",
    "    ORD = {\n",
    "        \"primer\": 1, \"primero\": 1, \"primera\": 1, \"1er\": 1, \"1ro\": 1, \"1ra\": 1,\n",
    "        \"segundo\": 2, \"segunda\": 2, \"2do\": 2, \"2da\": 2,\n",
    "        \"tercero\": 3, \"tercera\": 3, \"tercer\": 3, \"3er\": 3, \"3ro\": 3, \"3ra\": 3,\n",
    "        \"cuarto\": 4, \"cuarta\": 4, \"4to\": 4, \"4ta\": 4,\n",
    "        \"quinto\": 5, \"quinta\": 5, \"5to\": 5, \"5ta\": 5,\n",
    "    }\n",
    "    t = norm(text)\n",
    "    m = re.search(r\"\\bpiso\\s+(\\d+)\\b\", t) or re.search(r\"\\b(\\d+)\\s*(?:do|da|º|°|o|a)?\\s+piso\\b\", t)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    for w, n in ORD.items():\n",
    "        if re.search(rf\"\\bpiso\\s+{re.escape(w)}\\b\", t) or re.search(rf\"\\b{re.escape(w)}\\s+piso\\b\", t):\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def extract_person_query(text: str) -> str:\n",
    "    t = norm(text)\n",
    "    t = re.sub(r\"\\b(ing\\.?|msc\\.?|phd\\.?|dr\\.?)\\b\", \" \", t)\n",
    "    stop = {\n",
    "        \"que\",\"sabe\",\"saber\",\"sabes\",\"sobre\",\"acerca\",\"del\",\"de\",\"la\",\"el\",\"los\",\"las\",\"en\",\n",
    "        \"cuanto\",\"cuantos\",\"cuantas\",\"laboratorio\",\"lab\",\"laboratorios\",\"espacio\",\"espacios\",\n",
    "        \"aula\",\"aulas\",\"sala\",\"salas\",\"quien\",\"quienes\",\"esta\",\"encargado\",\"encargada\",\n",
    "    }\n",
    "    toks = [tok for tok in re.findall(r\"[a-z0-9]+\", t) if tok not in stop]\n",
    "    if len(toks) >= 2:\n",
    "        return \" \".join(toks[-3:])\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def parse_facets_universal(query: str, docs: List[Dict[str, Any]], attr_catalog: Dict[str, set]) -> Dict[str, Any]:\n",
    "    qn = norm(query)\n",
    "    facets: Dict[str, Any] = {}\n",
    "    toks = re.findall(r\"[a-záéíóúñ\\.]+\", qn)\n",
    "\n",
    "    tipos = []\n",
    "    for tok in toks:\n",
    "        base = TIPO_SYNONYMS.get(tok, None)\n",
    "        if base and base not in tipos:\n",
    "            tipos.append(base)\n",
    "    if tipos:\n",
    "        facets[\"tipo\"] = tipos\n",
    "\n",
    "    mb = re.search(r\"\\b(?:bloque|edificio)\\s*([a-z]?\\d+)\\b\", qn)\n",
    "    if mb:\n",
    "        b = mb.group(1).upper()\n",
    "        if b.isdigit():\n",
    "            b = \"A\" + b\n",
    "        facets[\"bloque\"] = b\n",
    "\n",
    "    p = parse_piso(qn)\n",
    "    if p is not None:\n",
    "        facets[\"piso\"] = p\n",
    "\n",
    "    q_persona = extract_person_query(qn)\n",
    "    people = set()\n",
    "    if q_persona:\n",
    "        for d in docs:\n",
    "            for nm in (d.get(\"encargados\") or []):\n",
    "                if nm and jaccard_tokens(nm, q_persona) >= 0.3:\n",
    "                    people.add(nm)\n",
    "    if people:\n",
    "        facets[\"encargado\"] = sorted(people)\n",
    "\n",
    "    carreras = set()\n",
    "    facults = set()\n",
    "    for d in docs:\n",
    "        car = d.get(\"carrera\")\n",
    "        if isinstance(car, list):\n",
    "            for x in car:\n",
    "                if norm(x) in qn:\n",
    "                    carreras.add(x)\n",
    "        elif car and norm(car) in qn:\n",
    "            carreras.add(car)\n",
    "        fac = d.get(\"facultad\")\n",
    "        if fac and norm(fac) in qn:\n",
    "            facults.add(fac)\n",
    "    if carreras:\n",
    "        facets[\"carrera\"] = sorted(carreras)\n",
    "    if facults:\n",
    "        facets[\"facultad\"] = sorted(facults)\n",
    "\n",
    "    return facets\n",
    "\n",
    "# ============================ híbrido ============================\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    top_k: int = 12\n",
    "    pool_k: int = 250\n",
    "    bm25_cand: int = 500\n",
    "    emb_cand: int = 300\n",
    "    min_score_bm25: float = 0.0\n",
    "    min_score_emb: float = 0.0\n",
    "    ctx_chars: int = 2800  # default\n",
    "    use_embeddings: bool = True\n",
    "    use_reranker: bool = True\n",
    "    use_facets: bool = True\n",
    "    generate_enabled: bool = True\n",
    "    faiss_M: int = 32\n",
    "    faiss_ef_search: int = 150\n",
    "    rerank_top_k: int = 150\n",
    "    rerank_alpha: float = 0.85\n",
    "    rerank_bonus_weight: float = 0.15\n",
    "    kind_prior_weight: float = 0.08\n",
    "    person_prior_weight: float = 0.12\n",
    "    enable_fuzzy_bonus: bool = True\n",
    "    fuzzy_min_sim: float = 0.30\n",
    "    min_best_emb_for_any: float = 0.45\n",
    "    min_emb_gap: float = 0.05\n",
    "    facet_mode: str = \"soft\"   # soft | strict\n",
    "    strict_keep_min: int = 3\n",
    "    debug: bool = False\n",
    "\n",
    "def build_bm25_candidates(qn: str, bm25: BM25Okapi, k: int, min_score: float, cfg: RAGConfig, docs: List[Dict[str, Any]]) -> List[Tuple[int, float]]:\n",
    "    toks = tokenize_simple(qn)\n",
    "    if not toks:\n",
    "        return []\n",
    "    scores = bm25.get_scores(toks)\n",
    "    if float(np.max(scores)) <= 0.0:\n",
    "        return []\n",
    "    if cfg.enable_fuzzy_bonus and toks:\n",
    "        tokq = \" \".join(toks)\n",
    "        bonus = np.zeros_like(scores, dtype=\"float32\")\n",
    "        for i, d in enumerate(docs):\n",
    "            smax = 0.0\n",
    "            smax = max(smax, jaccard_trigram(tokq, d.get(\"_name_norm\", \"\")))\n",
    "            smax = max(smax, jaccard_trigram(tokq, norm(d.get(\"codigo\") or \"\")))\n",
    "            if smax >= cfg.fuzzy_min_sim:\n",
    "                bonus[i] = 0.15 * smax\n",
    "        scores = scores + bonus\n",
    "\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    out = []\n",
    "    for i in order:\n",
    "        s = float(scores[i])\n",
    "        if s < min_score:\n",
    "            continue\n",
    "        out.append((int(i), s))\n",
    "        if len(out) >= k:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def build_embeddings_candidates(query: str, emb_model, emb_index, k: int, min_score: float, cfg: RAGConfig) -> List[Tuple[int, float]]:\n",
    "    if emb_index is None or emb_model is None:\n",
    "        return []\n",
    "    qn = norm(query.strip())\n",
    "    q = (\"query: \" + qn) if \"e5\" in EMB_MODEL_NAME.lower() else qn\n",
    "    qv = emb_model.encode([q], normalize_embeddings=True, show_progress_bar=False)[0].astype(\"float32\")\n",
    "    import faiss as _faiss\n",
    "    _faiss.normalize_L2(qv.reshape(1, -1))\n",
    "    D, I = emb_index.search(qv.reshape(1, -1), k)\n",
    "    return [(int(idx), float(s)) for idx, s in zip(I[0].tolist(), D[0].tolist()) if idx >= 0 and s >= min_score]\n",
    "\n",
    "def rrf_fuse(list_of_lists: List[List[Tuple[int, float]]], k: int, k_rrf: int = 60) -> List[int]:\n",
    "    ranks: Dict[int, float] = {}\n",
    "    for l in list_of_lists:\n",
    "        for r, (idx, _) in enumerate(l):\n",
    "            ranks[idx] = ranks.get(idx, 0.0) + 1.0 / (k_rrf + r + 1.0)\n",
    "    fused = sorted(ranks.items(), key=lambda kv: -kv[1])\n",
    "    return [i for i, _ in fused[:k]]\n",
    "\n",
    "def snippet_compacto(d: Dict[str, Any]) -> Optional[str]:\n",
    "    if d.get(\"kind\") != \"espacio\":\n",
    "        return None\n",
    "    partes = []\n",
    "    if d.get(\"codigo\"):\n",
    "        partes.append(f\"Código={d['codigo']}\")\n",
    "    _, name_display = dedup_type_name(d.get(\"tipo\"), d.get(\"nombre\"))\n",
    "    if name_display:\n",
    "        partes.append(f\"Nombre={name_display}\")\n",
    "\n",
    "    ubic = []\n",
    "    if d.get(\"bloque\"):\n",
    "        ubic.append(f\"Bloque {d['bloque']}\")\n",
    "    if d.get(\"piso\") is not None:\n",
    "        ubic.append(f\"Piso {d['piso']}\")\n",
    "    if ubic:\n",
    "        partes.append(\"Ubicación=\" + \" ; \".join(ubic))\n",
    "\n",
    "    dirp = []\n",
    "    if d.get(\"direccionRelativa\"):\n",
    "        dirp.append(d[\"direccionRelativa\"])\n",
    "    if d.get(\"direccionOrientativa\"):\n",
    "        dirp.append(d[\"direccionOrientativa\"])\n",
    "    if dirp:\n",
    "        partes.append(\"Dirección=\" + \" ; \".join(dirp))\n",
    "\n",
    "    if d.get(\"carrera\"):\n",
    "        partes.append(\"Carrera=\" + (\"; \".join(d[\"carrera\"]) if isinstance(d[\"carrera\"], list) else str(d[\"carrera\"])))\n",
    "    if d.get(\"facultad\"):\n",
    "        partes.append(f\"Facultad={d['facultad']}\")\n",
    "    if d.get(\"capacidad\"):\n",
    "        partes.append(f\"Capacidad={d['capacidad']}\")\n",
    "    if d.get(\"encargados\"):\n",
    "        partes.append(\"Encargados=\" + \"; \".join(d[\"encargados\"]))\n",
    "\n",
    "    return \" | \".join(partes)\n",
    "\n",
    "def lexical_bonus(q: str, d: Dict[str, Any]) -> float:\n",
    "    qn = norm(q)\n",
    "    name = d.get(\"_name_norm\") or \"\"\n",
    "    code = norm(d.get(\"codigo\") or \"\")\n",
    "    return max([jaccard_trigram(qn, name), jaccard_trigram(qn, code)])\n",
    "\n",
    "def value_matches(doc_val, wanted_list_norm: List[str]) -> bool:\n",
    "    if not wanted_list_norm:\n",
    "        return True\n",
    "    if doc_val is None:\n",
    "        return False\n",
    "    cand = [norm(x) for x in (doc_val if isinstance(doc_val, list) else [doc_val])]\n",
    "    wanted = set(norm(v) for v in wanted_list_norm)\n",
    "    return any(v in wanted for v in cand)\n",
    "\n",
    "def doc_facet_match_score(d: Dict[str, Any], facets: Dict[str, Any]) -> float:\n",
    "    wants = 0\n",
    "    hits = 0\n",
    "\n",
    "    if \"tipo\" in facets:\n",
    "        wants += 1\n",
    "        hits += int(norm(d.get(\"tipo\")) in [norm(x) for x in facets[\"tipo\"]])\n",
    "\n",
    "    if \"bloque\" in facets:\n",
    "        wants += 1\n",
    "        hits += int(norm(d.get(\"bloque\")) == norm(facets[\"bloque\"]))\n",
    "\n",
    "    if \"piso\" in facets:\n",
    "        wants += 1\n",
    "        try:\n",
    "            hits += int(int(d.get(\"piso\")) == int(facets[\"piso\"]))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if \"carrera\" in facets:\n",
    "        wants += 1\n",
    "        hits += int(value_matches(d.get(\"carrera\"), facets[\"carrera\"]))\n",
    "\n",
    "    if \"facultad\" in facets:\n",
    "        wants += 1\n",
    "        hits += int(value_matches(d.get(\"facultad\"), facets[\"facultad\"]))\n",
    "\n",
    "    if \"encargado\" in facets:\n",
    "        wants += 1\n",
    "        encs = [norm(x) for x in (d.get(\"encargados\") or [])]\n",
    "        qenc = [norm(x) for x in facets[\"encargado\"]]\n",
    "        hits += int(any(x in encs for x in qenc))\n",
    "\n",
    "    if wants == 0:\n",
    "        return 0.0\n",
    "    return hits / wants\n",
    "\n",
    "# ============================ engine ============================\n",
    "\n",
    "class RAGEngine:\n",
    "    def __init__(self, cfg: RAGConfig = None, evaluation_mode: bool = False):\n",
    "        self.cfg = cfg or RAGConfig()\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "\n",
    "        self.docs: List[Dict[str, Any]] = []\n",
    "        self.attr_catalog: Dict[str, set] = {}\n",
    "        self.bm25 = None\n",
    "        self.emb_model = None\n",
    "        self.emb_index = None\n",
    "        self.reranker = None\n",
    "        self.llm = None\n",
    "\n",
    "        self.prompt_agent = Path(PROMPT_PATH_AGENT).read_text(encoding=\"utf-8\") if Path(PROMPT_PATH_AGENT).exists() else \"{{HECHOS}}\\n\\nPregunta: {{PREGUNTA}}\"\n",
    "        self.prompt_eval  = Path(PROMPT_PATH_EVAL).read_text(encoding=\"utf-8\") if Path(PROMPT_PATH_EVAL).exists() else \"{{HECHOS}}\\n\\nPregunta: {{PREGUNTA}}\"\n",
    "\n",
    "        self.last_evidence_map: Dict[str, Dict[str, Any]] = {}\n",
    "        self.last_context_eids: List[str] = []\n",
    "        self.last_facets: Dict[str, Any] = {}\n",
    "        self.last_retriever_by_doc: Dict[int, str] = {}\n",
    "\n",
    "        self.rebuild_all()\n",
    "\n",
    "    # --- toggles con carga/descarga segura ---\n",
    "    def set_reranker_enabled(self, enabled: bool) -> str:\n",
    "        enabled = bool(enabled)\n",
    "        self.cfg.use_reranker = enabled\n",
    "        if not enabled:\n",
    "            self.reranker = None\n",
    "            return \"[OK] rerank=OFF\"\n",
    "        if self.reranker is None:\n",
    "            try:\n",
    "                self.reranker = CrossEncoder(RERANKER_MODEL, device=DEVICE)\n",
    "            except Exception as e:\n",
    "                self.cfg.use_reranker = False\n",
    "                self.reranker = None\n",
    "                return f\"[ERR] no se pudo cargar reranker: {e}\"\n",
    "        return \"[OK] rerank=ON\"\n",
    "\n",
    "    def set_facets_enabled(self, enabled: bool) -> str:\n",
    "        self.cfg.use_facets = bool(enabled)\n",
    "        return f\"[OK] faceted={'ON' if self.cfg.use_facets else 'OFF'}\"\n",
    "\n",
    "    def rebuild_all(self):\n",
    "        t0 = time.time()\n",
    "        self.docs = load_docs(JSON_PATH)\n",
    "        self.attr_catalog = collect_attr_catalog(self.docs)\n",
    "        self.bm25 = build_bm25(self.docs)\n",
    "\n",
    "        if self.cfg.use_embeddings:\n",
    "            self.emb_model, self.emb_index = build_embeddings(self.docs, EMB_MODEL_NAME, M=self.cfg.faiss_M)\n",
    "            try:\n",
    "                if hasattr(self.emb_index, \"hnsw\"):\n",
    "                    self.emb_index.hnsw.efSearch = self.cfg.faiss_ef_search\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            self.emb_model, self.emb_index = None, None\n",
    "\n",
    "        if self.cfg.use_reranker:\n",
    "            try:\n",
    "                self.reranker = CrossEncoder(RERANKER_MODEL, device=DEVICE)\n",
    "            except Exception:\n",
    "                self.reranker = None\n",
    "                self.cfg.use_reranker = False\n",
    "        else:\n",
    "            self.reranker = None\n",
    "\n",
    "        if Llama is not None and Path(GGUF_PATH).exists():\n",
    "            self.llm = Llama(\n",
    "                model_path=str(GGUF_PATH),\n",
    "                n_ctx=N_CTX,\n",
    "                n_threads=N_THREADS,\n",
    "                n_gpu_layers=N_GPU_LAYERS,\n",
    "                logits_all=False,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.llm = None\n",
    "\n",
    "        print(f\"[OK] Índices listos en {time.time() - t0:.2f}s. Docs: {len(self.docs)} | LLM={'ON' if self.llm else 'OFF'} | Rerank={'ON' if self.cfg.use_reranker else 'OFF'}\")\n",
    "\n",
    "    def pin_exact_ids(self, query: str) -> List[int]:\n",
    "        q_raw = query or \"\"\n",
    "        qn = norm(q_raw)\n",
    "        q_codes = set(t.lower() for t in CODE_RE.findall(q_raw))\n",
    "        hits = []\n",
    "        for i, d in enumerate(self.docs):\n",
    "            code = (d.get(\"codigo\") or \"\").lower().strip()\n",
    "            if code and code in q_codes:\n",
    "                hits.append(i)\n",
    "                continue\n",
    "            for al in (d.get(\"_aliases_norm\") or []):\n",
    "                if al and (al in qn or jaccard_trigram(al, qn) >= 0.82):\n",
    "                    hits.append(i)\n",
    "                    break\n",
    "        return list(dict.fromkeys(hits))\n",
    "\n",
    "    def retrieve_pool(self, query: str):\n",
    "        qn = norm(query)\n",
    "        bm25_list = build_bm25_candidates(qn, self.bm25, self.cfg.bm25_cand, self.cfg.min_score_bm25, self.cfg, self.docs)\n",
    "        emb_list = build_embeddings_candidates(query, self.emb_model, self.emb_index, self.cfg.emb_cand, self.cfg.min_score_emb, self.cfg) if self.cfg.use_embeddings else []\n",
    "\n",
    "        bm25_set = {idx for idx, _ in bm25_list}\n",
    "        emb_set  = {idx for idx, _ in emb_list}\n",
    "\n",
    "        retr: Dict[int, str] = {}\n",
    "        for idx in (bm25_set | emb_set):\n",
    "            in_bm25 = idx in bm25_set\n",
    "            in_emb  = idx in emb_set\n",
    "            if in_bm25 and in_emb:\n",
    "                retr[idx] = \"BM25 + E5+FAISS\"\n",
    "            elif in_bm25:\n",
    "                retr[idx] = \"BM25\"\n",
    "            else:\n",
    "                retr[idx] = \"E5+FAISS\"\n",
    "\n",
    "        emb_scores = [s for _, s in emb_list]\n",
    "        best_emb = max(emb_scores, default=0.0)\n",
    "        if emb_scores:\n",
    "            sorted_emb = sorted(emb_scores, reverse=True)\n",
    "            median_emb = sorted_emb[len(sorted_emb)//2]\n",
    "            gap = best_emb - median_emb\n",
    "        else:\n",
    "            median_emb, gap = 0.0, 0.0\n",
    "\n",
    "        if (not bm25_list) and emb_list:\n",
    "            if best_emb < self.cfg.min_best_emb_for_any or gap < self.cfg.min_emb_gap:\n",
    "                return [], {}, [], {\"bm25_n\": len(bm25_list), \"emb_n\": len(emb_list), \"best_emb\": best_emb, \"median_emb\": median_emb, \"gap\": gap}\n",
    "\n",
    "        if not bm25_list and not emb_list:\n",
    "            return [], {}, [], {\"bm25_n\": 0, \"emb_n\": 0, \"best_emb\": 0.0, \"median_emb\": 0.0, \"gap\": 0.0}\n",
    "\n",
    "        fused_ids = rrf_fuse([bm25_list, emb_list], k=self.cfg.pool_k, k_rrf=60)\n",
    "        pins = self.pin_exact_ids(query)\n",
    "        for idx in pins:\n",
    "            retr[idx] = retr.get(idx, \"BM25\")\n",
    "        pool = list(dict.fromkeys(pins + fused_ids))[: self.cfg.pool_k]\n",
    "\n",
    "        facets = parse_facets_universal(query, self.docs, self.attr_catalog) if self.cfg.use_facets else {}\n",
    "\n",
    "        self.last_retriever_by_doc = retr\n",
    "        stats = {\"bm25_n\": len(bm25_list), \"emb_n\": len(emb_list), \"pins_n\": len(pins), \"pool_n\": len(pool)}\n",
    "        return pool, facets, pins, stats\n",
    "\n",
    "    def rerank_order(self, query: str, idxs: List[int]) -> List[int]:\n",
    "        if not self.cfg.use_reranker or not self.reranker or not idxs:\n",
    "            return idxs\n",
    "        R = min(self.cfg.rerank_top_k, len(idxs))\n",
    "        to_rerank = idxs[:R]\n",
    "        rest = idxs[R:]\n",
    "\n",
    "        pairs = [(query, self.docs[i].get(\"_raw_text\", \"\")) for i in to_rerank]\n",
    "        s = np.asarray(self.reranker.predict(pairs, batch_size=(32 if DEVICE == \"cuda\" else 8)), dtype=\"float32\")\n",
    "        ce = (s - s.min()) / (s.max() - s.min() + 1e-8) if len(s) > 1 else np.array([1.0], dtype=\"float32\")\n",
    "\n",
    "        bonus = np.zeros_like(ce)\n",
    "        prior = np.zeros_like(ce)\n",
    "        for k, idx in enumerate(to_rerank):\n",
    "            d = self.docs[idx]\n",
    "            bonus[k] = lexical_bonus(query, d)\n",
    "            prior[k] = self.cfg.kind_prior_weight\n",
    "\n",
    "        final = self.cfg.rerank_alpha * ce + self.cfg.rerank_bonus_weight * bonus + prior\n",
    "        order = np.argsort(final)[::-1]\n",
    "        return [to_rerank[i] for i in order] + rest\n",
    "\n",
    "    def apply_facets_mode(self, ranked_pool: List[int], facets: Dict[str, Any], pins: List[int]) -> List[int]:\n",
    "        if not facets or (not self.cfg.use_facets):\n",
    "            return list(dict.fromkeys(pins + ranked_pool))\n",
    "\n",
    "        scored = [(i, doc_facet_match_score(self.docs[i], facets)) for i in ranked_pool]\n",
    "        scored.sort(key=lambda kv: kv[1], reverse=True)\n",
    "        ordered = [i for i, _ in scored]\n",
    "\n",
    "        if self.cfg.facet_mode == \"soft\":\n",
    "            return list(dict.fromkeys(pins + ordered))\n",
    "\n",
    "        strict = [i for i, s in scored if s >= 0.999]\n",
    "        strict = list(dict.fromkeys(pins + strict))\n",
    "        if len(strict) >= max(1, self.cfg.strict_keep_min):\n",
    "            return strict\n",
    "        return list(dict.fromkeys(pins + ordered))\n",
    "\n",
    "    def build_context_with_evidence(self, ordered_idxs: List[int], facets: Dict[str, Any]) -> str:\n",
    "        self.last_evidence_map = {}\n",
    "        self.last_context_eids = []\n",
    "        self.last_facets = dict(facets or {})\n",
    "\n",
    "        ctx_lines = []\n",
    "        total = 0\n",
    "        eid = 1\n",
    "\n",
    "        max_docs = max(1, int(self.cfg.top_k))\n",
    "        for idx in ordered_idxs[:max_docs]:\n",
    "            s = snippet_compacto(self.docs[idx])\n",
    "            if not s:\n",
    "                continue\n",
    "            line = f\"[E{eid}] {s}\"\n",
    "            if total + len(line) + 1 > int(self.cfg.ctx_chars):\n",
    "                break\n",
    "\n",
    "            d = self.docs[idx]\n",
    "            key = f\"E{eid}\"\n",
    "            self.last_evidence_map[key] = {\n",
    "                \"eid\": key,\n",
    "                \"doc_idx\": idx,\n",
    "                \"snippet\": s,\n",
    "                \"codigo\": d.get(\"codigo\"),\n",
    "                \"nombre\": d.get(\"nombre\"),\n",
    "                \"tipo\": d.get(\"tipo\"),\n",
    "                \"bloque\": d.get(\"bloque\"),\n",
    "                \"piso\": d.get(\"piso\"),\n",
    "                \"retriever\": self.last_retriever_by_doc.get(idx, \"—\"),\n",
    "            }\n",
    "            self.last_context_eids.append(key)\n",
    "            ctx_lines.append(line)\n",
    "            total += len(line) + 1\n",
    "            eid += 1\n",
    "\n",
    "        head = \"\"\n",
    "        if (not self.evaluation_mode) and facets:\n",
    "            head = f\"[FACETS] activos={list(facets.keys())}\\n\"\n",
    "        return head + \"\\n\".join(ctx_lines)\n",
    "\n",
    "    def build_prompt(self, context_text: str, query: str) -> str:\n",
    "        tpl = self.prompt_eval if self.evaluation_mode else self.prompt_agent\n",
    "        return tpl.replace(\"{{HECHOS}}\", context_text).replace(\"{{PREGUNTA}}\", query)\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.llm:\n",
    "            return \"Gen OFF (LLM no disponible).\"\n",
    "        temp = TEMPERATURE_EVAL if self.evaluation_mode else TEMPERATURE_AGENT\n",
    "        out = self.llm(prompt=prompt, max_tokens=MAX_TOKENS, temperature=temp, top_p=1.0, repeat_penalty=1.1)\n",
    "        return out[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    def audit_evidence(self, eids: List[str], facets: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "        out = {}\n",
    "        want_bloque = norm(facets.get(\"bloque\")) if facets.get(\"bloque\") else None\n",
    "        want_piso = facets.get(\"piso\", None)\n",
    "        want_tipos = [norm(x) for x in facets.get(\"tipo\", [])] if facets.get(\"tipo\") else []\n",
    "\n",
    "        for eid in eids:\n",
    "            m = self.last_evidence_map.get(eid, {})\n",
    "            ok = True\n",
    "            reasons = []\n",
    "\n",
    "            if want_bloque and norm(m.get(\"bloque\")) != want_bloque:\n",
    "                ok = False\n",
    "                reasons.append(f\"bloque={m.get('bloque')}≠{facets.get('bloque')}\")\n",
    "\n",
    "            if want_piso is not None and m.get(\"piso\") is not None:\n",
    "                try:\n",
    "                    if int(m.get(\"piso\")) != int(want_piso):\n",
    "                        ok = False\n",
    "                        reasons.append(f\"piso={m.get('piso')}≠{want_piso}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if want_tipos and norm(m.get(\"tipo\")) not in want_tipos:\n",
    "                ok = False\n",
    "                reasons.append(f\"tipo={m.get('tipo')}∉{facets.get('tipo')}\")\n",
    "\n",
    "            out[eid] = {\"ok\": ok, \"reasons\": reasons}\n",
    "        return out\n",
    "\n",
    "    def _append_citations_inline(self, text: str, used: List[str]) -> str:\n",
    "        used = [u for u in used if u in self.last_evidence_map]\n",
    "        if not used:\n",
    "            return (text or \"\").strip()\n",
    "        cites = \" \".join([f\"[{u}]\" for u in used])\n",
    "        t = (text or \"\").rstrip()\n",
    "\n",
    "        if t.endswith(cites):\n",
    "            return t\n",
    "\n",
    "        t = re.sub(r\"(\\s*(\\[[eE]\\d+\\]\\s*)+)$\", \"\", t).rstrip()\n",
    "        return (t + (\"\\n\\n\" if t else \"\") + cites).strip()\n",
    "\n",
    "    def answer_stream(self, query: str):\n",
    "        q = (query or \"\").strip()\n",
    "        if not q:\n",
    "            yield {\"status\": \"Escribe una consulta.\", \"final\": True, \"answer\": \"Escribe una consulta.\", \"evidence_table\": [], \"facets\": {}}\n",
    "            return\n",
    "\n",
    "        yield {\"status\": \"1/5 Normalizando consulta…\", \"final\": False}\n",
    "        yield {\"status\": \"2/5 Recuperando candidatos (BM25/embeddings)…\", \"final\": False}\n",
    "        pool, facets, pins, _stats = self.retrieve_pool(q)\n",
    "\n",
    "        if not pool and not pins:\n",
    "            yield {\n",
    "                \"status\": \"Listo. (sin resultados)\",\n",
    "                \"final\": True,\n",
    "                \"answer\": \"No consta en el contexto lo que se pregunta.\",\n",
    "                \"evidence_table\": [],\n",
    "                \"facets\": facets,\n",
    "            }\n",
    "            return\n",
    "\n",
    "        yield {\"status\": \"3/5 Rerank + facetas…\", \"final\": False}\n",
    "        ranked = self.rerank_order(q, pool)\n",
    "        ordered = self.apply_facets_mode(ranked, facets, pins)\n",
    "\n",
    "        yield {\"status\": \"4/5 Construyendo evidencias…\", \"final\": False}\n",
    "        ctx = self.build_context_with_evidence(ordered, facets)\n",
    "\n",
    "        if not self.cfg.generate_enabled:\n",
    "            used = self.last_context_eids[: min(8, len(self.last_context_eids))]\n",
    "            audit = self.audit_evidence(used, facets) if facets else {eid: {\"ok\": True, \"reasons\": []} for eid in used}\n",
    "            table = []\n",
    "            for eid in used:\n",
    "                m = self.last_evidence_map.get(eid, {})\n",
    "                a = audit.get(eid, {\"ok\": True, \"reasons\": []})\n",
    "                table.append({\n",
    "                    \"EID\": eid,\n",
    "                    \"OK\": \"✅\" if a[\"ok\"] else \"⚠\",\n",
    "                    \"Motivos\": \"; \".join(a[\"reasons\"]) if a[\"reasons\"] else \"\",\n",
    "                    \"Retriever\": m.get(\"retriever\"),\n",
    "                    \"Bloque\": m.get(\"bloque\"),\n",
    "                    \"Piso\": m.get(\"piso\"),\n",
    "                    \"Tipo\": m.get(\"tipo\"),\n",
    "                    \"Código\": m.get(\"codigo\"),\n",
    "                    \"Nombre\": m.get(\"nombre\"),\n",
    "                    \"Snippet\": m.get(\"snippet\"),\n",
    "                })\n",
    "\n",
    "            ans = self._append_citations_inline(ctx, used)\n",
    "            yield {\"status\": \"Listo. (gen OFF)\", \"final\": True, \"answer\": ans, \"evidence_table\": table, \"facets\": facets}\n",
    "            return\n",
    "\n",
    "        yield {\"status\": \"5/5 Generando respuesta (LLM)…\", \"final\": False}\n",
    "        prompt = self.build_prompt(ctx, q)\n",
    "        text = self.generate(prompt)\n",
    "\n",
    "        used_nums = sorted(set(re.findall(r\"\\bE(\\d+)\\b\", text)))\n",
    "        used = [f\"E{n}\" for n in used_nums if f\"E{n}\" in self.last_evidence_map]\n",
    "        if not used:\n",
    "            used = self.last_context_eids[: min(8, len(self.last_context_eids))]\n",
    "\n",
    "        audit = self.audit_evidence(used, facets) if facets else {eid: {\"ok\": True, \"reasons\": []} for eid in used}\n",
    "        table = []\n",
    "        for eid in used:\n",
    "            m = self.last_evidence_map.get(eid, {})\n",
    "            a = audit.get(eid, {\"ok\": True, \"reasons\": []})\n",
    "            table.append({\n",
    "                \"EID\": eid,\n",
    "                \"OK\": \"✅\" if a[\"ok\"] else \"⚠\",\n",
    "                \"Motivos\": \"; \".join(a[\"reasons\"]) if a[\"reasons\"] else \"\",\n",
    "                \"Retriever\": m.get(\"retriever\"),\n",
    "                \"Bloque\": m.get(\"bloque\"),\n",
    "                \"Piso\": m.get(\"piso\"),\n",
    "                \"Tipo\": m.get(\"tipo\"),\n",
    "                \"Código\": m.get(\"codigo\"),\n",
    "                \"Nombre\": m.get(\"nombre\"),\n",
    "                \"Snippet\": m.get(\"snippet\"),\n",
    "            })\n",
    "\n",
    "        text = self._append_citations_inline(text, used)\n",
    "        yield {\"status\": \"Listo.\", \"final\": True, \"answer\": text, \"evidence_table\": table, \"facets\": facets}\n",
    "\n",
    "# ============================ comandos (para GUI) ============================\n",
    "\n",
    "HELP_TEXT = \"\"\"Comandos:\n",
    "  :help               -> muestra ayuda\n",
    "  :mode agent|eval    -> modo agente / evaluación\n",
    "  :facet soft|strict  -> faceted soft o strict\n",
    "  :facets on|off      -> encender/apagar faceted (uso de facetas)\n",
    "  :rerank on|off      -> encender/apagar rerank (CrossEncoder)\n",
    "  :k N                -> top_k\n",
    "  :gen on|off         -> LLM ON/OFF\n",
    "  :exit               -> marcar salida (no apaga servidor; engine queda cargado)\n",
    "\"\"\"\n",
    "\n",
    "def parse_command(line: str) -> Tuple[str, List[str]]:\n",
    "    parts = line.strip().split()\n",
    "    cmd = parts[0].lower()\n",
    "    args = parts[1:]\n",
    "    return cmd, args\n",
    "\n",
    "def _parse_on_off(s: str) -> Optional[bool]:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    if s in {\"on\", \"1\", \"true\", \"si\", \"sí\", \"enable\", \"enabled\"}:\n",
    "        return True\n",
    "    if s in {\"off\", \"0\", \"false\", \"no\", \"disable\", \"disabled\"}:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "def handle_command(engine: RAGEngine, line: str, ui: str = \"gui\") -> str:\n",
    "    cmd, args = parse_command(line)\n",
    "\n",
    "    if cmd == \":help\":\n",
    "        return HELP_TEXT\n",
    "\n",
    "    if cmd == \":config\":\n",
    "        return str(engine.cfg)\n",
    "\n",
    "    if cmd == \":rebuild\":\n",
    "        engine.rebuild_all()\n",
    "        return \"[OK] rebuild completo\"\n",
    "\n",
    "    if cmd == \":mode\" and args:\n",
    "        v = args[0].lower()\n",
    "        if v in {\"agent\", \"eval\"}:\n",
    "            engine.evaluation_mode = (v == \"eval\")\n",
    "            return f\"[OK] modo={'EVALUACIÓN' if engine.evaluation_mode else 'AGENTE'}\"\n",
    "        return \"[ERR] Uso: :mode agent|eval\"\n",
    "\n",
    "    if cmd == \":facet\" and args:\n",
    "        v = args[0].lower()\n",
    "        if v in {\"soft\", \"strict\"}:\n",
    "            engine.cfg.facet_mode = v\n",
    "            return f\"[OK] facet_mode={v}\"\n",
    "        return \"[ERR] Uso: :facet soft|strict\"\n",
    "\n",
    "    if cmd == \":facets\" and args:\n",
    "        flag = _parse_on_off(args[0])\n",
    "        if flag is None:\n",
    "            return \"[ERR] Uso: :facets on|off\"\n",
    "        return engine.set_facets_enabled(flag)\n",
    "\n",
    "    if cmd == \":rerank\" and args:\n",
    "        flag = _parse_on_off(args[0])\n",
    "        if flag is None:\n",
    "            return \"[ERR] Uso: :rerank on|off\"\n",
    "        return engine.set_reranker_enabled(flag)\n",
    "\n",
    "    if cmd == \":k\" and args and args[0].isdigit():\n",
    "        engine.cfg.top_k = int(args[0])\n",
    "        return f\"[OK] top_k={engine.cfg.top_k}\"\n",
    "\n",
    "    if cmd == \":gen\" and args:\n",
    "        flag = _parse_on_off(args[0])\n",
    "        if flag is None:\n",
    "            return \"[ERR] Uso: :gen on|off\"\n",
    "        engine.cfg.generate_enabled = flag\n",
    "        return f\"[OK] gen={'ON' if engine.cfg.generate_enabled else 'OFF'}\"\n",
    "\n",
    "    if cmd == \":exit\":\n",
    "        return \"__EXIT__\"\n",
    "\n",
    "    return \"[ERR] Comando no reconocido. Usa :help\"\n",
    "\n",
    "# ============================ GUI ============================\n",
    "\n",
    "GUI_CSS = \"\"\"\n",
    "#app-title { font-weight: 800; font-size: 18px; margin: 0 0 6px 0; }\n",
    "#app-sub { opacity: .8; font-size: 12px; margin: 0 0 10px 0; }\n",
    "\"\"\"\n",
    "\n",
    "def launch_gui(share: bool = False, server_port: int = 7860):\n",
    "    global engine\n",
    "    if engine is None:\n",
    "        engine = RAGEngine(cfg=RAGConfig())\n",
    "\n",
    "    def hist_append_messages(hist, user_msg=None, bot_msg=None):\n",
    "        h = list(hist or [])\n",
    "        if user_msg is not None:\n",
    "            h.append({\"role\": \"user\", \"content\": str(user_msg)})\n",
    "        if bot_msg is not None:\n",
    "            h.append({\"role\": \"assistant\", \"content\": str(bot_msg)})\n",
    "        return h\n",
    "\n",
    "    def set_last_assistant(hist, content: str):\n",
    "        h = list(hist or [])\n",
    "        if h and h[-1].get(\"role\") == \"assistant\":\n",
    "            h[-1][\"content\"] = str(content)\n",
    "        else:\n",
    "            h.append({\"role\": \"assistant\", \"content\": str(content)})\n",
    "        return h\n",
    "\n",
    "    def run_stream(user_in, hist, facet_mode_in, top_k_in, detail_in, gen_in):\n",
    "        t = (user_in or \"\").strip()\n",
    "        if not t:\n",
    "            yield hist, \"**Estado:** Escribe una consulta.\", [], \"\"\n",
    "            return\n",
    "\n",
    "        if t.startswith(\":\"):\n",
    "            msg = handle_command(engine, t, ui=\"gui\")\n",
    "            if msg == \"__EXIT__\":\n",
    "                msg = \"Sesión marcada como finalizada. Puedes cerrar la pestaña. (engine queda cargado.)\"\n",
    "            h = hist_append_messages(hist, user_msg=t, bot_msg=msg)\n",
    "            yield h, \"**Estado:** Listo.\", [], \"\"\n",
    "            return\n",
    "\n",
    "        engine.cfg.facet_mode = str(facet_mode_in)\n",
    "        engine.cfg.top_k = int(top_k_in)\n",
    "        engine.cfg.generate_enabled = bool(gen_in)\n",
    "        engine.cfg.ctx_chars = clamp_int(detail_in, SAFE_CTX_MIN, SAFE_CTX_MAX, engine.cfg.ctx_chars)\n",
    "\n",
    "        h = hist_append_messages(hist, user_msg=t, bot_msg=\"Procesando…\")\n",
    "        yield h, \"**Estado:** Iniciando…\", [], f\"Detalle efectivo: {engine.cfg.ctx_chars} chars (cap={SAFE_CTX_MAX})\"\n",
    "\n",
    "        last_rows = []\n",
    "        for step in engine.answer_stream(t):\n",
    "            status = step.get(\"status\", \"…\")\n",
    "            is_final = bool(step.get(\"final\", False))\n",
    "\n",
    "            if not is_final:\n",
    "                yield h, f\"**Estado:** {status}\", last_rows, f\"Detalle efectivo: {engine.cfg.ctx_chars} chars (cap={SAFE_CTX_MAX})\"\n",
    "                continue\n",
    "\n",
    "            ans = (step.get(\"answer\") or \"\").strip()\n",
    "            facets = step.get(\"facets\") or {}\n",
    "            if facets:\n",
    "                ans = ans + f\"\\n\\nFacetas: {facets}\"\n",
    "\n",
    "            h = set_last_assistant(h, ans)\n",
    "\n",
    "            table = (step.get(\"evidence_table\") or [])[: int(engine.cfg.top_k)]\n",
    "            rows = []\n",
    "            for r in table:\n",
    "                rows.append([\n",
    "                    str(r.get(\"EID\",\"\")),\n",
    "                    str(r.get(\"OK\",\"\")),\n",
    "                    str(r.get(\"Motivos\",\"\")),\n",
    "                    str(r.get(\"Retriever\",\"\")),\n",
    "                    str(r.get(\"Bloque\",\"\")),\n",
    "                    str(r.get(\"Piso\",\"\")),\n",
    "                    str(r.get(\"Tipo\",\"\")),\n",
    "                    str(r.get(\"Código\",\"\")),\n",
    "                    str(r.get(\"Nombre\",\"\")),\n",
    "                ])\n",
    "\n",
    "            last_rows = rows\n",
    "            yield h, f\"**Estado:** {status}\", rows, f\"Detalle efectivo: {engine.cfg.ctx_chars} chars (cap={SAFE_CTX_MAX})\"\n",
    "\n",
    "    with gr.Blocks(title=\"Agente LLM de Localización de espacios del Edificio 2 - UNL\") as demo:\n",
    "        gr.HTML(f\"<style>{GUI_CSS}</style>\")\n",
    "        gr.Markdown(\"## Agente LLM de Localización de espacios del Edificio 2 - UNL\")\n",
    "        gr.Markdown(\"Pregunta → respuesta con citas. Ayuda: `:help`\")\n",
    "\n",
    "        state_hist = gr.State([])\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                try:\n",
    "                    chat = gr.Chatbot(label=\"Chat\", height=360, type=\"messages\")\n",
    "                except TypeError:\n",
    "                    chat = gr.Chatbot(label=\"Chat\", height=360)\n",
    "\n",
    "                user_text = gr.Textbox(label=\"Escribe tu consulta\", lines=2)\n",
    "                with gr.Row():\n",
    "                    btn_send = gr.Button(\"Enviar\")\n",
    "                    btn_clear = gr.Button(\"Limpiar\")\n",
    "\n",
    "                status = gr.Markdown(\"**Estado:** Listo.\")\n",
    "                detail_eff = gr.Markdown(f\"Detalle efectivo: {RAGConfig().ctx_chars} chars (cap={SAFE_CTX_MAX})\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                with gr.Accordion(\"Configuración\", open=True):\n",
    "                    facet_mode = gr.Radio(choices=[\"soft\", \"strict\"], value=\"soft\", label=\"Faceted (modo)\")\n",
    "                    top_k = gr.Slider(1, 30, value=12, step=1, label=\"Top-K (evidencias)\")\n",
    "                    detail = gr.Slider(800, 6000, value=2800, step=100, label=\"Detalle (evidencias)\")\n",
    "                    gen_on = gr.Checkbox(value=True, label=\"Generar con LLM\")\n",
    "\n",
    "                gr.Markdown(\"### Evidencias usadas\")\n",
    "                ev_df = gr.Dataframe(\n",
    "                    headers=[\"EID\",\"OK\",\"Motivos\",\"Retriever\",\"Bloque\",\"Piso\",\"Tipo\",\"Código\",\"Nombre\"],\n",
    "                    datatype=[\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n",
    "                    row_count=0,\n",
    "                    col_count=(9, \"fixed\"),\n",
    "                    interactive=False,\n",
    "                )\n",
    "\n",
    "        def clear_all():\n",
    "            return [], [], \"**Estado:** Listo.\", [], f\"Detalle efectivo: {engine.cfg.ctx_chars if engine else RAGConfig().ctx_chars} chars (cap={SAFE_CTX_MAX})\"\n",
    "\n",
    "        btn_send.click(\n",
    "            run_stream,\n",
    "            inputs=[user_text, state_hist, facet_mode, top_k, detail, gen_on],\n",
    "            outputs=[chat, status, ev_df, detail_eff],\n",
    "        )\n",
    "        user_text.submit(\n",
    "            run_stream,\n",
    "            inputs=[user_text, state_hist, facet_mode, top_k, detail, gen_on],\n",
    "            outputs=[chat, status, ev_df, detail_eff],\n",
    "        )\n",
    "\n",
    "        def sync_hist(chat_value):\n",
    "            return chat_value\n",
    "\n",
    "        chat.change(sync_hist, inputs=[chat], outputs=[state_hist])\n",
    "\n",
    "        btn_clear.click(\n",
    "            clear_all,\n",
    "            outputs=[chat, state_hist, status, ev_df, detail_eff],\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        demo.queue(concurrency_count=1, max_size=64)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    demo.launch(share=share, server_port=server_port)\n",
    "\n",
    "# ============================ boot ============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    launch_gui(share=False, server_port=7860)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f3936-0998-4fd7-bb4b-ce6076ec22cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] Casos cargados: 69\n",
      "[TIMING][build_embeddings] total=18.131s\n",
      "[OK] Índices listos en 23.55s. Docs: 26 | LLM=ON | Rerank=ON\n",
      "\n",
      "==================== Caso 1 / 69 ====================\n",
      "ID: NOMBRE_A211\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A211?\n",
      "GOLD: Laboratorio Integrado de Manufactura.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 2 / 69 ====================\n",
      "ID: NOMBRE_A212\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A212?\n",
      "GOLD: Laboratorio de Energia y Fluidos.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 3 / 69 ====================\n",
      "ID: NOMBRE_A221\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A221?\n",
      "GOLD: Laboratorio de Desarrollo de Software.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 4 / 69 ====================\n",
      "ID: NOMBRE_A222\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A222?\n",
      "GOLD: Laboratorio de Automatización y Control.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 5 / 69 ====================\n",
      "ID: NOMBRE_A223\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A223?\n",
      "GOLD: Laboratorio de Sistemas Automotrices.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 6 / 69 ====================\n",
      "ID: NOMBRE_A224\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A224?\n",
      "GOLD: Laboratorio de Instalaciones Eléctricas.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 7 / 69 ====================\n",
      "ID: NOMBRE_A225\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A225?\n",
      "GOLD: Laboratorio de Computación Aplicada.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 8 / 69 ====================\n",
      "ID: NOMBRE_A231\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A231?\n",
      "GOLD: Laboratorio de Cómputo, Redes y Sistemas Operativos.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 9 / 69 ====================\n",
      "ID: NOMBRE_A232\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A232?\n",
      "GOLD: Laboratorio de Antenas y Telecomunicaciones.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 10 / 69 ====================\n",
      "ID: NOMBRE_A234\n",
      "PREGUNTA: ¿Cómo se llama el laboratorio A234?\n",
      "GOLD: Laboratorio de Electrónica.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 11 / 69 ====================\n",
      "ID: NOMBRE_DEP_P4\n",
      "PREGUNTA: ¿Cómo se llama el departamento del piso 4?\n",
      "GOLD: Departamento de Investigacion e Innovación Tecnológica - I2TEC.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 12 / 69 ====================\n",
      "ID: NOMBRE_SALA_ELEC\n",
      "PREGUNTA: ¿Cómo se llama la sala donde se dicta docencia de Electromecánica y Eletrónica y Telecomunicaciones?\n",
      "GOLD: Sala de Docencia-Carrera de Electromecánica y Eletrónica y Telecomunicaciones.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 13 / 69 ====================\n",
      "ID: UBICACION_A211\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A211?\n",
      "GOLD: Bloque A2, piso 1, lado derecho.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 1.0000\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 14 / 69 ====================\n",
      "ID: UBICACION_A212\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A212?\n",
      "GOLD: Bloque A2, piso 1, lado derecho.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 0.8000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9857\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 15 / 69 ====================\n",
      "ID: UBICACION_A21B1\n",
      "PREGUNTA: ¿Dónde queda el baño A21B1?\n",
      "GOLD: Bloque A2, piso 1, lado izquierdo.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9867\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 16 / 69 ====================\n",
      "ID: UBICACION_A21B2\n",
      "PREGUNTA: ¿Dónde queda el baño A21B2?\n",
      "GOLD: Bloque A2, piso 1, lado izquierdo.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 0.8000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 1.0000\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9750\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 1.0000\n",
      "\n",
      "==================== Caso 17 / 69 ====================\n",
      "ID: UBICACION_A221\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A221?\n",
      "GOLD: Bloque A2, piso 2, lado derecho.\n",
      "ROUGE-1 precision: 0.0465\n",
      "ROUGE-1 F1       : 0.0889\n",
      "Faithfulness     : 0.5735\n",
      "TOKENS NO RESPALDADOS: ['response', 'pregunta', 'cual', 'response', 'pregunta', 'pertenece', 'response', 'pregunta', 'cual', 'response', 'pregunta', 'ascensor', 'response', 'pregunta', 'pertenece', 'response', 'pregunta', 'llego', 'desde', 'a1', 'response', 've', 'sube', 'pregunta', 'pertenecen', 'response', 'pregunta', 'encargado', 'labor']\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 0.9439\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9229\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 0.9749\n",
      "\n",
      "==================== Caso 18 / 69 ====================\n",
      "ID: UBICACION_A225\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A225?\n",
      "GOLD: Bloque A2, piso 2, lado izquierdo.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 0.9470\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9272\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 0.9763\n",
      "\n",
      "==================== Caso 19 / 69 ====================\n",
      "ID: UBICACION_A231\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A231?\n",
      "GOLD: Bloque A2, piso 3, lado derecho.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 0.9498\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9310\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 0.9776\n",
      "\n",
      "==================== Caso 20 / 69 ====================\n",
      "ID: UBICACION_A232\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A232?\n",
      "GOLD: Bloque A2, piso 3, lado derecho.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 1.0000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 0.9523\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9344\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 0.9787\n",
      "\n",
      "==================== Caso 21 / 69 ====================\n",
      "ID: UBICACION_A234\n",
      "PREGUNTA: ¿Dónde queda el laboratorio A234?\n",
      "GOLD: Bloque A2, piso 3, lado izquierdo.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 0.8000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 0.9546\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9280\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 0.9797\n",
      "\n",
      "==================== Caso 22 / 69 ====================\n",
      "ID: UBICACION_A24B1\n",
      "PREGUNTA: ¿Dónde queda el baño A24B1?\n",
      "GOLD: Bloque A2, piso 4, lado izquierdo.\n",
      "ROUGE-1 precision: 1.0000\n",
      "ROUGE-1 F1       : 0.8000\n",
      "Faithfulness     : 1.0000\n",
      "----------------------------------------------\n",
      "PROMEDIO ROUGE-1 PRECISION HASTA AHORA: 0.9567\n",
      "PROMEDIO ROUGE-1 F1 HASTA AHORA       : 0.9222\n",
      "PROMEDIO FAITHFULNESS HASTA AHORA     : 0.9806\n",
      "\n",
      "==================== Caso 23 / 69 ====================\n",
      "ID: UBICACION_LAB_ELECT\n",
      "PREGUNTA: ¿Dónde queda el Laboratorio de Electrónica?\n",
      "GOLD: Bloque A2, piso 3, lado izquierdo.\n"
     ]
    }
   ],
   "source": [
    "# ==================== BLOQUE 2: Evaluación (ROUGE-1 + Faithfulness) ====================\n",
    "# Compatible con tu engine actual (RAGEngine(cfg=..., evaluation_mode=...))\n",
    "# Requiere que este bloque se ejecute DESPUÉS de haber definido RAGConfig, RAGEngine, DEVICE, JSON_PATH, etc.\n",
    "\n",
    "import json, csv, unicodedata, re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Any\n",
    "\n",
    "# -------------------- archivo de evaluación --------------------\n",
    "EVAL_FILE = \"eval.jsonl\"\n",
    "\n",
    "CSV_OUT_DETALLE        = \"resultados_eval_detallado.csv\"\n",
    "CSV_OUT_RESUMEN_GLOBAL = \"resultados_eval_resumen_global.csv\"\n",
    "CSV_OUT_RESUMEN_TIPO   = \"resultados_eval_resumen_por_tipo.csv\"\n",
    "CSV_OUT_DEBUG          = \"resultados_eval_debug.csv\"  # opcional (incluye ctx)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Carga de casos de evaluación\n",
    "# ---------------------------------------------------------\n",
    "eval_items = []\n",
    "with open(EVAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        eval_items.append(json.loads(line))\n",
    "\n",
    "print(f\"[EVAL] Casos cargados: {len(eval_items)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Instanciar motor en modo evaluación\n",
    "# ---------------------------------------------------------\n",
    "# NOTA:\n",
    "# - Si quieres evaluar el \"sistema final\", pon use_reranker=True y use_facets=True\n",
    "# - Si quieres evaluar sin esos módulos (ablation), deja False como abajo.\n",
    "cfg_eval = RAGConfig(\n",
    "    top_k=12,\n",
    "    pool_k=250,\n",
    "    bm25_cand=500,\n",
    "    emb_cand=300,\n",
    "    min_score_bm25=0.0,\n",
    "    min_score_emb=0.0,\n",
    "    ctx_chars=2200,\n",
    "    use_embeddings=True,\n",
    "    use_reranker=True,     # <- cambia a True si evalúas con reranker\n",
    "    use_facets=True,       # <- cambia a True si evalúas con facetas\n",
    "    generate_enabled=True,  # <- True para generar con LLM\n",
    "    rerank_top_k=(150 if DEVICE == \"cuda\" else 60),\n",
    "    rerank_alpha=0.85,\n",
    "    rerank_bonus_weight=0.15,\n",
    "    kind_prior_weight=0.08,\n",
    "    person_prior_weight=0.12,\n",
    "    faiss_M=32,\n",
    "    faiss_ef_search=150,\n",
    "    facet_mode=\"strict\",\n",
    "    strict_keep_min=3,      # <- (antes lo tenías como min_faceted; en tu engine es strict_keep_min)\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "engine_eval = RAGEngine(cfg=cfg_eval, evaluation_mode=True)\n",
    "\n",
    "if not getattr(engine_eval, \"llm\", None):\n",
    "    print(\"[WARN] LLM no disponible (no se cargó GGUF / llama_cpp). La evaluación generará 'Gen OFF'.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Utilidades de normalización + ROUGE-1 clásico\n",
    "# ---------------------------------------------------------\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    return \"\".join(c for c in s if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalización ligera para ROUGE-1:\n",
    "    - lower\n",
    "    - sin acentos\n",
    "    - sin puntuación (deja letras/números/espacios)\n",
    "    - colapsa espacios\n",
    "    - quita 'respuesta:' al inicio (si aparece)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    t = str(text).strip().lower()\n",
    "    t = strip_accents(t)\n",
    "    t = re.sub(r\"^respuesta\\s*:\\s*\", \"\", t)\n",
    "    t = re.sub(r\"[^a-z0-9áéíóúñ]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def rouge1_scores(pred: str, gold: str):\n",
    "    \"\"\"\n",
    "    ROUGE-1 clásico (unigrams):\n",
    "    Devuelve: precision, recall, f1, pred_only_tokens, gold_only_tokens\n",
    "    \"\"\"\n",
    "    ng = normalize_basic(gold)\n",
    "    npred = normalize_basic(pred)\n",
    "\n",
    "    gold_tokens = ng.split() if ng else []\n",
    "    pred_tokens = npred.split() if npred else []\n",
    "\n",
    "    if not gold_tokens and not pred_tokens:\n",
    "        return 1.0, 1.0, 1.0, [], []\n",
    "    if not gold_tokens or not pred_tokens:\n",
    "        return 0.0, 0.0, 0.0, pred_tokens, gold_tokens\n",
    "\n",
    "    gold_counts = Counter(gold_tokens)\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "\n",
    "    overlap = 0\n",
    "    for tok, c in pred_counts.items():\n",
    "        overlap += min(c, gold_counts.get(tok, 0))\n",
    "\n",
    "    precision = overlap / sum(pred_counts.values()) if pred_counts else 0.0\n",
    "    recall    = overlap / sum(gold_counts.values()) if gold_counts else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "\n",
    "    pred_only = sorted((pred_counts - gold_counts).elements())\n",
    "    gold_only = sorted((gold_counts - pred_counts).elements())\n",
    "\n",
    "    return precision, recall, f1, pred_only, gold_only\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Faithfulness = cobertura de evidencia en contexto\n",
    "# ---------------------------------------------------------\n",
    "STOPWORDS = {\n",
    "    \"el\",\"la\",\"los\",\"las\",\"un\",\"una\",\"unos\",\"unas\",\n",
    "    \"de\",\"del\",\"al\",\"a\",\"y\",\"o\",\"u\",\"en\",\"por\",\"para\",\"con\",\n",
    "    \"que\",\"qué\",\"quien\",\"quién\",\"donde\",\"dónde\",\"como\",\"cómo\",\n",
    "    \"es\",\"son\",\"está\",\"estan\",\"esta\",\"están\",\n",
    "    \"laboratorio\",\"laboratorios\",\"sala\",\"salas\",\"departamento\",\"departamentos\",\n",
    "    \"baño\",\"banio\",\"banos\",\"banios\",\"cuarto\",\"cuartos\",\n",
    "    \"piso\",\"pisos\",\"bloque\",\"bloques\",\"lado\",\"lados\",\n",
    "    \"tiene\",\"hay\",\"se\",\"encuentra\",\"queda\",\n",
    "    \"si\",\"sí\",\"no\",\"respuesta\"\n",
    "}\n",
    "\n",
    "def tokenize_content(text: str):\n",
    "    t = normalize_basic(text)\n",
    "    toks = t.split()\n",
    "    return [tok for tok in toks if tok not in STOPWORDS]\n",
    "\n",
    "def faithfulness_score(pred: str, context: str):\n",
    "    \"\"\"\n",
    "    Faithfulness simple:\n",
    "    score = (#tokens_de_respuesta_que_aparecen_en_contexto) / (#tokens_de_respuesta)\n",
    "    Devuelve: score, supported_tokens, unsupported_tokens\n",
    "    \"\"\"\n",
    "    ans_tokens = tokenize_content(pred)\n",
    "    ctx_tokens = set(tokenize_content(context))\n",
    "\n",
    "    if not ans_tokens:\n",
    "        return 1.0, [], []\n",
    "\n",
    "    supported = [t for t in ans_tokens if t in ctx_tokens]\n",
    "    unsupported = [t for t in ans_tokens if t not in ctx_tokens]\n",
    "    score = len(supported) / len(ans_tokens) if ans_tokens else 0.0\n",
    "    return score, supported, unsupported\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Bucle de evaluación (ejecuta pipeline real del engine)\n",
    "# ---------------------------------------------------------\n",
    "global_rouge_f1_sum = 0.0\n",
    "global_rouge_p_sum  = 0.0\n",
    "global_faith_sum    = 0.0\n",
    "n_cases = 0\n",
    "\n",
    "per_type_scores = defaultdict(list)  # type -> list[(precision, f1, faith)]\n",
    "rows_for_csv = []\n",
    "debug_rows_for_csv = []\n",
    "\n",
    "for idx, item in enumerate(eval_items, start=1):\n",
    "    qid   = item.get(\"id\", f\"case_{idx}\")\n",
    "    q     = item.get(\"question\") or item.get(\"query\") or \"\"\n",
    "    gold  = item.get(\"gold\", \"\")\n",
    "    qtype = item.get(\"type\", \"OTRO\")\n",
    "\n",
    "    print(f\"\\n==================== Caso {idx} / {len(eval_items)} ====================\")\n",
    "    print(f\"ID: {qid}\")\n",
    "    print(f\"PREGUNTA: {q}\")\n",
    "    print(f\"GOLD: {gold}\")\n",
    "\n",
    "    # ---- pipeline manual para capturar contexto + pred ----\n",
    "    pool, facets, pins, _stats = engine_eval.retrieve_pool(q)\n",
    "    ranked = engine_eval.rerank_order(q, pool)\n",
    "    ordered = engine_eval.apply_facets_mode(ranked, facets, pins)\n",
    "    ctx = engine_eval.build_context_with_evidence(ordered, facets)\n",
    "    prompt = engine_eval.build_prompt(ctx, q)\n",
    "    pred = engine_eval.generate(prompt)\n",
    "\n",
    "    # ---- métricas ----\n",
    "    r_p, r_r, r_f1, pred_only, gold_only = rouge1_scores(pred, gold)\n",
    "    faith, supported_tokens, unsupported_tokens = faithfulness_score(pred, ctx)\n",
    "\n",
    "    global_rouge_f1_sum += r_f1\n",
    "    global_rouge_p_sum  += r_p\n",
    "    global_faith_sum    += faith\n",
    "    n_cases += 1\n",
    "\n",
    "    per_type_scores[qtype].append((r_p, r_f1, faith))\n",
    "\n",
    "    print(f\"ROUGE-1 precision: {r_p:.4f}\")\n",
    "    print(f\"ROUGE-1 F1       : {r_f1:.4f}\")\n",
    "    print(f\"Faithfulness     : {faith:.4f}\")\n",
    "    if unsupported_tokens:\n",
    "        print(f\"TOKENS NO RESPALDADOS: {unsupported_tokens}\")\n",
    "\n",
    "    avg_r_f1 = global_rouge_f1_sum / n_cases\n",
    "    avg_r_p  = global_rouge_p_sum  / n_cases\n",
    "    avg_f    = global_faith_sum    / n_cases\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"PROMEDIO ROUGE-1 PRECISION HASTA AHORA: {avg_r_p:.4f}\")\n",
    "    print(f\"PROMEDIO ROUGE-1 F1 HASTA AHORA       : {avg_r_f1:.4f}\")\n",
    "    print(f\"PROMEDIO FAITHFULNESS HASTA AHORA     : {avg_f:.4f}\")\n",
    "\n",
    "    rows_for_csv.append({\n",
    "        \"id\": qid,\n",
    "        \"type\": qtype,\n",
    "        \"question\": q,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"rouge1_precision\": r_p,\n",
    "        \"rouge1_recall\": r_r,\n",
    "        \"rouge1_f1\": r_f1,\n",
    "        \"faithfulness\": faith,\n",
    "        \"tiene_tokens_no_respaldo\": int(len(unsupported_tokens) > 0),\n",
    "    })\n",
    "\n",
    "    debug_rows_for_csv.append({\n",
    "        \"id\": qid,\n",
    "        \"type\": qtype,\n",
    "        \"question\": q,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"rouge1_f1\": r_f1,\n",
    "        \"faithfulness\": faith,\n",
    "        \"rouge_pred_only_tokens\": \" \".join(pred_only),\n",
    "        \"rouge_gold_only_tokens\": \" \".join(gold_only),\n",
    "        \"faith_supported_tokens\": \" \".join(supported_tokens),\n",
    "        \"faith_unsupported_tokens\": \" \".join(unsupported_tokens),\n",
    "        \"context_used\": ctx,\n",
    "        \"facets_detected\": json.dumps(facets, ensure_ascii=False),\n",
    "        \"stats\": json.dumps(_stats, ensure_ascii=False),\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Resumen global y por tipo\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n==================== RESUMEN FINAL ====================\")\n",
    "print(f\"Casos evaluados: {n_cases}\")\n",
    "avg_rouge_f1 = global_rouge_f1_sum / n_cases if n_cases else 0.0\n",
    "avg_rouge_p  = global_rouge_p_sum  / n_cases if n_cases else 0.0\n",
    "avg_faith    = global_faith_sum    / n_cases if n_cases else 0.0\n",
    "print(f\"ROUGE-1 precision promedio: {avg_rouge_p:.4f}\")\n",
    "print(f\"ROUGE-1 F1 promedio       : {avg_rouge_f1:.4f}\")\n",
    "print(f\"Faithfulness promedio     : {avg_faith:.4f}\")\n",
    "\n",
    "resumen_global = [{\n",
    "    \"casos_evaluados\": n_cases,\n",
    "    \"rouge1_precision_promedio\": avg_rouge_p,\n",
    "    \"rouge1_f1_promedio\": avg_rouge_f1,\n",
    "    \"faithfulness_promedio\": avg_faith,\n",
    "}]\n",
    "\n",
    "print(\"\\n==================== PROMEDIOS POR TIPO ====================\")\n",
    "resumen_por_tipo = []\n",
    "for t, vals in per_type_scores.items():\n",
    "    if not vals:\n",
    "        continue\n",
    "    p_mean = sum(v[0] for v in vals) / len(vals)\n",
    "    f1_mean = sum(v[1] for v in vals) / len(vals)\n",
    "    faith_mean = sum(v[2] for v in vals) / len(vals)\n",
    "    resumen_por_tipo.append({\n",
    "        \"type\": t,\n",
    "        \"casos\": len(vals),\n",
    "        \"rouge1_precision_promedio\": p_mean,\n",
    "        \"rouge1_f1_promedio\": f1_mean,\n",
    "        \"faithfulness_promedio\": faith_mean,\n",
    "    })\n",
    "    print(\n",
    "        f\"Tipo {t}: casos={len(vals)} | \"\n",
    "        f\"ROUGE-1 precision={p_mean:.4f} | \"\n",
    "        f\"ROUGE-1 F1={f1_mean:.4f} | \"\n",
    "        f\"Faithfulness={faith_mean:.4f}\"\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Guardar CSVs\n",
    "# ---------------------------------------------------------\n",
    "if rows_for_csv:\n",
    "    detalle_fields = list(rows_for_csv[0].keys())\n",
    "    with open(CSV_OUT_DETALLE, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=detalle_fields)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows_for_csv)\n",
    "    print(f\"[EVAL] Resultados detallados guardados en: {CSV_OUT_DETALLE}\")\n",
    "\n",
    "if resumen_global:\n",
    "    with open(CSV_OUT_RESUMEN_GLOBAL, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        fieldnames = list(resumen_global[0].keys())\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(resumen_global)\n",
    "    print(f\"[EVAL] Resumen global guardado en: {CSV_OUT_RESUMEN_GLOBAL}\")\n",
    "\n",
    "if resumen_por_tipo:\n",
    "    with open(CSV_OUT_RESUMEN_TIPO, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        fieldnames = list(resumen_por_tipo[0].keys())\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(resumen_por_tipo)\n",
    "    print(f\"[EVAL] Resumen por tipo guardado en: {CSV_OUT_RESUMEN_TIPO}\")\n",
    "\n",
    "if debug_rows_for_csv:\n",
    "    debug_fields = list(debug_rows_for_csv[0].keys())\n",
    "    with open(CSV_OUT_DEBUG, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=debug_fields)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(debug_rows_for_csv)\n",
    "    print(f\"[EVAL] Debug detallado guardado en: {CSV_OUT_DEBUG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c44fe43-f57c-442c-b29e-5e184ea22bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV mejorado generado en: resultados_evaluacion_final_mejorado.csv\n"
     ]
    }
   ],
   "source": [
    "#Limpieza de csv para el análisis estadístico\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "CSV_IN  = \"resultados_eval_detallado.csv\"\n",
    "CSV_OUT = \"resultados_evaluacion_final_mejorado.csv\"\n",
    "\n",
    "CUT_MARKERS = [\n",
    "    r\"\\bPREGUNTA\\s*:\\s*\",\n",
    "    r\"\\bHECHOS\\s*:\\s*\",\n",
    "    r\"\\bFACETS\\s*:\\s*\",\n",
    "    r\"\\bCONTEXTO\\s*:\\s*\",\n",
    "]\n",
    "\n",
    "def normalize_unicode(s: str) -> str:\n",
    "    # Normaliza unicode y elimina controles raros\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = re.sub(r\"[\\u0000-\\u001F\\u007F]\", \" \", s)  # control chars\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def clean_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    s = str(x)\n",
    "\n",
    "    # 1) Unifica saltos de línea\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\n+\", \" \", s)\n",
    "\n",
    "    # 2) Si hubo “fuga” del prompt (PREGUNTA/HECHOS/etc), corta desde ahí\n",
    "    for m in CUT_MARKERS:\n",
    "        hit = re.search(m, s, flags=re.IGNORECASE)\n",
    "        if hit:\n",
    "            s = s[:hit.start()].strip()\n",
    "\n",
    "    # 3) Limpieza estética: separadores “;” -> coma (más humano)\n",
    "    s = re.sub(r\"\\s*;\\s*\", \", \", s)\n",
    "\n",
    "    # 4) Quita prefijos típicos\n",
    "    s = re.sub(r\"^(respuesta\\s*:\\s*)\", \"\", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # 5) Normaliza unicode/espacios finales\n",
    "    s = normalize_unicode(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "rows_out = []\n",
    "with open(CSV_IN, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        pred_raw = row.get(\"pred\", \"\")\n",
    "        gold_raw = row.get(\"gold\", \"\")\n",
    "\n",
    "        pred = clean_text(pred_raw)\n",
    "        gold = clean_text(gold_raw)\n",
    "\n",
    "        # bandera simple de “fuga” (por si quieres reportarlo como limitación)\n",
    "        leak = int(bool(re.search(r\"(PREGUNTA\\s*:|HECHOS\\s*:|FACETS\\s*:)\", str(pred_raw), flags=re.IGNORECASE)))\n",
    "\n",
    "        r_f1 = float(row.get(\"rouge1_f1\", 0) or 0)\n",
    "        faith = float(row.get(\"faithfulness\", 0) or 0)\n",
    "\n",
    "        rows_out.append({\n",
    "            \"id\": clean_text(row.get(\"id\")),\n",
    "            \"tipo_pregunta\": clean_text(row.get(\"type\")),\n",
    "            \"pregunta\": clean_text(row.get(\"question\")),\n",
    "            \"respuesta_referencia\": gold,\n",
    "            \"respuesta_agente\": pred,\n",
    "            # métricas (dos formatos: 0-1 y %)\n",
    "            \"rouge1_f1\": round(r_f1, 4),\n",
    "            \"rouge1_f1_pct\": round(100.0 * r_f1, 2),\n",
    "            \"faithfulness\": round(faith, 4),\n",
    "            \"faithfulness_pct\": round(100.0 * faith, 2),\n",
    "            \"prompt_leak\": leak,\n",
    "        })\n",
    "\n",
    "# (Opcional) ordena por id para que sea más presentable\n",
    "rows_out.sort(key=lambda r: (r[\"id\"] or \"\"))\n",
    "\n",
    "with open(CSV_OUT, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"id\",\"tipo_pregunta\",\"pregunta\",\n",
    "            \"respuesta_referencia\",\"respuesta_agente\",\n",
    "            \"rouge1_f1\",\"rouge1_f1_pct\",\n",
    "            \"faithfulness\",\"faithfulness_pct\",\n",
    "            \"prompt_leak\"\n",
    "        ],\n",
    "        delimiter=\";\"\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "print(f\"CSV mejorado generado en: {CSV_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84cc0ec5-9c35-4166-9a2e-31e0b582d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 69\n",
      "Media ROUGE-1 F1 = 0.8762\n",
      "Desviación estándar = 0.3157\n",
      "IC 95% = [0.8018, 0.9507]\n"
     ]
    }
   ],
   "source": [
    "#Calculo de la media, desviación estandar e IC a 95%\n",
    "import csv, math\n",
    "\n",
    "PATH = \"resultados_evaluacion_final_mejorado.csv\"\n",
    "\n",
    "vals = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\";\")\n",
    "    for r in reader:\n",
    "        vals.append(float(r[\"rouge1_f1\"]))\n",
    "\n",
    "n = len(vals)\n",
    "mean = sum(vals) / n\n",
    "var = sum((x - mean)**2 for x in vals) / n\n",
    "std = math.sqrt(var)\n",
    "\n",
    "z = 1.96\n",
    "ci_low = mean - z * std / math.sqrt(n)\n",
    "ci_high = mean + z * std / math.sqrt(n)\n",
    "\n",
    "print(f\"N = {n}\")\n",
    "print(f\"Media ROUGE-1 F1 = {mean:.4f}\")\n",
    "print(f\"Desviación estándar = {std:.4f}\")\n",
    "print(f\"IC 95% = [{ci_low:.4f}, {ci_high:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b91fc-897d-4a17-95be-cb72ed0cc779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (GPU)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
